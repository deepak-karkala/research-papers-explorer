[
  {
    "id": "paper001", "title": "Attention Is All You Need", "authors": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "Lukasz Kaiser", "Illia Polosukhin"], "year": 2017, "venue": "NeurIPS",
    "pdf_url": "https://arxiv.org/abs/1706.03762", "code_url": "https://github.com/tensorflow/tensor2tensor", "abstract": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
    "takeaways": ["Introduced the Transformer model", "Relies entirely on self-attention mechanisms", "No recurrence or convolutions needed", "Achieved SOTA in machine translation"],
    "thumbnail_image_url": "images/papers/paper001_thumb.png", "artist_id": "artist001", "genre_ids": ["genre001"]
  },
  {
    "id": "paper002", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "authors": ["Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova"], "year": 2018, "venue": "NAACL",
    "pdf_url": "https://arxiv.org/abs/1810.04805", "code_url": "https://github.com/google-research/bert", "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
    "takeaways": ["Introduced BERT model", "Bidirectional pre-training", "Fine-tunable for various NLP tasks", "Achieved SOTA on multiple NLP benchmarks"],
    "thumbnail_image_url": "images/papers/paper002_thumb.png", "artist_id": "artist001", "genre_ids": ["genre001"]
  },
  {
    "id": "paper003", "title": "ImageNet Classification with Deep Convolutional Neural Networks", "authors": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E. Hinton"], "year": 2012, "venue": "NeurIPS",
    "pdf_url": "https://papers.nips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf", "code_url": "https://code.google.com/archive/p/cuda-convnet/", "abstract": "We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0%, respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully-connected layers we employed a recently-developed regularization method called dropout that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry.",
    "takeaways": ["Popularized deep convolutional neural networks (AlexNet)", "Won ImageNet LSVRC-2012", "Used ReLU and dropout"],
    "thumbnail_image_url": "images/papers/paper003_thumb.png", "artist_id": "artist003", "genre_ids": ["genre002"]
  }
]
